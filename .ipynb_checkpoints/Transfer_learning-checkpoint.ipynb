{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a classifier using a pre-trained network\n",
    "In this kernel we will implement a transfer learning approach to classify the histopathological images. This means that we will not build from scratch our own network, but instead we will \"borrow\" the architecture and the weights from a pre-existing model, which has already been trained on some dataset. We will then adapt the weights to suit our own classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to make the plots visible\n",
    "%matplotlib inline\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.applications.nasnet import NASNetMobile, NASNetLarge, preprocess_input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "Build a dataframe with image ids and corresponding label (0 = no tumor tissue, 1 = has tumor tissue).\n",
    "The images are inside the `input` folder, subdivided in training and testing images. A `.csv` file in the `input` folder provides the labels corresponding to each image id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('D:/Kaggle/HistoPat/input/train_labels.csv')\n",
    "\n",
    "# removing this image because it caused a training error previously\n",
    "df_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "df_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n",
    "\n",
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the train and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.reset_index(drop=True)\n",
    "df_data = shuffle(df_data)\n",
    "\n",
    "# stratify=y creates a balanced validation set.\n",
    "y = df_data['label']\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=62, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "df_train['label'].value_counts()\n",
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the id as the index in df_data\n",
    "df_data.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created dataframes with only the image index and the corresponding labels. We won't load into memory all the images at the same time, to avoid the kernel dying. Instead, the images will be copied in different subfolders for training and validation, each with two subfolders corresponding to the two classes. The structure looks like this:\n",
    " - base_dir\n",
    "     - train_dir\n",
    "         - a_no_tumor_tissue\n",
    "         - b_has_tumor_tissue\n",
    "     - val_dir\n",
    "         - a_no_tumor_tissue\n",
    "         - b_has_tumor_tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'D:/Kaggle/HistoPat/base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "#create train_dir and val_dir\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "# create classes subfolders\n",
    "\n",
    "# create new folders inside train_dir\n",
    "no_tumor_tissue = os.path.join(train_dir, 'a_no_tumor_tissue')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(train_dir, 'b_has_tumor_tissue')\n",
    "os.mkdir(has_tumor_tissue)\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "no_tumor_tissue = os.path.join(val_dir, 'a_no_tumor_tissue')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(val_dir, 'b_has_tumor_tissue')\n",
    "os.mkdir(has_tumor_tissue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now transfer the images inside the respective folders (might take a while since here we are using all the images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "Now that our data is sorted the way we need it, we can perform some data augmentation. In this way the network will be trained with different versions of the same images (e.g. rotated images, mirrored images, zoom-ins, etc.), thus allowing for more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers some basic transformations via the `ImageDataGenerator` class. A wider set of possible transformations, which we will use here, is provided in the `imgaug` library https://github.com/aleju/imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'D:/Kaggle/HistoPat/base_dir/train_dir'\n",
    "valid_path = 'D:/Kaggle/HistoPat/base_dir/val_dir'\n",
    "test_path = 'D:/Kaggle/HistoPat/input/test'\n",
    "\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(0, 10)), # crop images from each side by 0 to 20px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images,\n",
    "    iaa.Flipud(0.3), # vertically flip 30% of the images,\n",
    "    iaa.Sometimes(0.5, iaa.OneOf([iaa.Sharpen(alpha=(0,0.2), lightness=(0.9,1.1)), iaa.Emboss(alpha=0.3, strength=0.3),\n",
    "                                 ]),\n",
    "                  iaa.OneOf([iaa.Dropout(0.12), iaa.SaltAndPepper(0.15)])\n",
    "                 )\n",
    "    ],\n",
    "    random_order=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine the augmenters above with a custom function that normalizes the pixels in the image, to ensure faster convergence of the network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_size = 96\n",
    "\n",
    "def normalize_image(x):\n",
    "    '''normalize pixel values for each image so that they have zero mean'''\n",
    "    if x.std() > 0:\n",
    "        return (x - x.mean())/x.std()\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def combined_augmentation(x):\n",
    "    '''\n",
    "    combine (one after the other) augmentation from imgaug with normalize_image(x)\n",
    "    '''\n",
    "    x = seq.augment_image(x)\n",
    "    x = normalize_image(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will still use the `ImageDataGenerator` class to create the generators, which will take batches of images from the folders created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(preprocessing_function=combined_augmentation)\n",
    "\n",
    "#for data validation we will only use the normalization, no augmentation\n",
    "datagen_val = ImageDataGenerator(preprocessing_function=normalize_image)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(img_size,img_size),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='binary')\n",
    "\n",
    "val_gen = datagen_val.flow_from_directory(valid_path,\n",
    "                                        target_size=(img_size,img_size),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='binary')\n",
    "\n",
    "# shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen_val.flow_from_directory(valid_path,\n",
    "                                        target_size=(img_size,img_size),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to observe the changes produced by the preprocessing functions on the images. Too severe modifications to the images might affect the performance of the model.\n",
    "We will take one image and check a few transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'D:/Kaggle/HistoPat/base_dir/train_dir/a_no_tumor_tissue/0000d563d5cfafc4e68acb7c9829258a298d9b6a.tif'\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_plot = ImageDataGenerator(preprocessing_function=seq.augment_image)\n",
    "# reshape image in the appropriate format\n",
    "img = img.reshape((1,)+img.shape)\n",
    "plot_gen = datagen_plot.flow(img)\n",
    "\n",
    "num_examples = 15\n",
    "for i in range(0, num_examples):                     \n",
    "    x_batch = next(plot_gen)\n",
    "    image = x_batch[0]\n",
    "    image = image/255.\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the modified versions of the image, which will be given in the training phase to the model. If the modifications are too heavy it could be a good idea to reduce the number or the strength of the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Instead of bulding a model from scratch, we will use a pre-exisiting architecture with pre-trained weights. Specifically here we chose the VGG-16 network (https://arxiv.org/pdf/1409.1556.pdf), trained on the ImageNet database, and at the end of the network we attach a couple of fully-connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_classif_vgg16():\n",
    "    \n",
    "    inputs = Input((96, 96, 3))\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "    x = base_model(inputs)\n",
    "    out1 = GlobalMaxPooling2D()(x)\n",
    "    out2 = GlobalAveragePooling2D()(x)\n",
    "    out3 = Flatten()(x)\n",
    "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.7)(out)    \n",
    "    out = Dense(512, activation='relu')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.7)(out)\n",
    "    out = Dense(256, activation='relu')(out)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Dropout(0.7)(out)\n",
    "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
    "    model = Model(inputs, out)\n",
    "    model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_classif_vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the VGG16 network is already pre-trained, while the last fully-connected layers are not, it is advisable to first train only the last layers, keeping the weights of the VGG16 part fixed, as in the first epochs the learning process might negatively affect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].trainable = False\n",
    "model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few training epochs we will unfreeze this part of the model and train everything together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compensate for the class imbalance we calculate the relative weights of the two classes and use it during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(df_train['label']),\n",
    "                                                 df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from keras import backend as K\n",
    "\n",
    "#epochs for training of only the last fully-connected layers\n",
    "epochs_tail = 3\n",
    "#epochs for training of the full model\n",
    "epochs_full = 6\n",
    "\n",
    "h5_path = \"weights_VGG16_TTA_v02_3tail6full.h5\"\n",
    "checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.2)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs_tail, verbose=1,\n",
    "    callbacks=[checkpoint, reducel],\n",
    "    steps_per_epoch=len(df_train) // train_batch_size,\n",
    "    validation_steps=len(df_val) // val_batch_size,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "#unfreeze VGG16 part and train all toghether\n",
    "model.layers[1].trainable = True\n",
    "#extract learning rate from training so far\n",
    "lr_sofar = K.get_value(model.optimizer.lr)\n",
    "model.compile(optimizer=Adam(lr_sofar), loss=binary_crossentropy, metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs_full, verbose=1,\n",
    "    callbacks=[checkpoint, reducel],\n",
    "    steps_per_epoch=len(df_train) // train_batch_size,\n",
    "    validation_steps=len(df_val) // val_batch_size,   \n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "#plot accuracies \n",
    "plt.plot(history.history['acc'], color='b')\n",
    "plt.plot(history.history['val_acc'], color='r')\n",
    "plt.title('Model Accuracies')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Accuracy', 'Validation accuracy'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "#plot losses\n",
    "plt.plot(history.history['loss'], color='b')\n",
    "plt.plot(history.history['val_loss'], color='r')\n",
    "plt.title('Model Losses')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss', 'Validation loss'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time for training: \" + str((end - start)/60) + \" minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "Calculate area under ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "model.load_weights('weights_VGG16_TTA_v02_3tail6full.h5')\n",
    "\n",
    "y_pred_val = model.predict_generator(test_gen, verbose=1, steps=len(df_val))\n",
    "fpr_keras, tpr_keras, _ = roc_curve(test_gen.classes, y_pred_val)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the predictions\n",
    "It is typically useful to try to understand where the model failed to correctly classify the images. To do so we can plot the \"mostly misclassified\" images, which means the images for which there is the largest difference between true and predicted label.\n",
    "\n",
    "Let's first have a look at the distribution of such difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = abs(y_pred_val - test_gen.classes.reshape(len(y_pred_val),1))\n",
    "\n",
    "h = plt.hist(y_diff, 30, range=(0.05,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram is obviously peaked at zero, since the model predicts the correct label for the vast majority of the validation images. For this reason we only plot it starting from differences of at least 0.05, in order to resolve the details of the rest of the plot. As it could be expected, there are less and less images as the difference between true and predicted labels increase. However we observe a sudden increase in the number of images for the last bin of the histogram. The reason for that is not clear. \n",
    "\n",
    "Now let's plot some of the images with the largest difference in labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclassified_images(num_img):\n",
    "    '''\n",
    "    Function to plot the images which received\n",
    "    the wrongest prediction label\n",
    "    '''\n",
    "    #create a vector of the absolute differences between true and predicted labels\n",
    "    y_diff = abs(y_pred_val - test_gen.classes.reshape(len(y_pred_val),1))\n",
    "    \n",
    "    #find indices of the num_img largest elements of y_diff\n",
    "    ind = np.argpartition(y_diff, -num_img, axis=0)[-num_img:]\n",
    "    \n",
    "    #from the indices obtain the corresponding images in the val_dir folder\n",
    "    names = []\n",
    "    paths = []\n",
    "    for i in range(0,len(ind)):\n",
    "        names.append(test_gen.filenames[int(ind[i])])\n",
    "        paths.append(names[i].replace(\"\\\\\",\"/\"))\n",
    "        paths[i] = 'D:/Kaggle/HistoPat/base_dir/val_dir/'+paths[i]\n",
    "        \n",
    "    #plot the images\n",
    "    fig, m_axs = plt.subplots(1, num_img, figsize=(18,2))\n",
    "    for ii, c_ax in enumerate(m_axs):\n",
    "        img = cv2.imread(paths[ii])\n",
    "        if paths[ii].find('a_no_tumor_tissue') > 0:\n",
    "            c_ax.set_title(\"True label: \\n 0\")\n",
    "        else:\n",
    "            c_ax.set_title(\"True label: \\n 1\")\n",
    "        c_ax.text(48, 155, 'Pred label: \\n '+str(np.round(y_pred_val[ind[ii]])), size=12, ha='center')\n",
    "        c_ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_misclassified_images(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To an untrained eye like mine it is hard to see any common features of all these images, which might have helped in understanding where and how the model is poorly performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on the test set\n",
    "Similarly to what has been done for the training phase, we define a folder structure also for the test images (with no subdivision in classes, since it is now known)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test_dir\n",
    "test_dir = 'D:/Kaggle/HistoPat/test_dir'\n",
    "os.mkdir(test_dir)\n",
    "    \n",
    "# create test_images inside test_dir\n",
    "test_images = os.path.join(test_dir, 'test_images')\n",
    "os.mkdir(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the test images into image_dir\n",
    "\n",
    "test_list = os.listdir('./input/test')\n",
    "\n",
    "for image in test_list:\n",
    "    \n",
    "    fname = image\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join('./input/test', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(test_images, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small trick which has proven extremely useful to improve the model's performance is Test-Time Augmentation (TTA). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
